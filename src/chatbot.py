import os
import numpy as np
import faiss
import streamlit as st
import nltk
from nltk.tokenize import sent_tokenize
from langchain.text_splitter import CharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from openai import OpenAI

# Baixa os pacotes necess√°rios para a tokeniza√ß√£o de senten√ßas
nltk.download('punkt')
nltk.download('punkt_tab')

# Configura o cliente OpenAI usando a chave de API do ambiente
client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY")
)

def preprocess_text(text):
    """
    Pr√©-processa o texto dividindo-o em senten√ßas.

    Args:
        text (str): Texto a ser pr√©-processado.

    Returns:
        list: Lista de senten√ßas extra√≠das do texto.
    """
    sentences = sent_tokenize(text)
    return sentences

def split_text(text):
    """
    Divide o texto em chunks menores para facilitar o processamento.

    Args:
        text (str): Texto a ser dividido.

    Returns:
        list: Lista de chunks de texto.
    """
    text_splitter = CharacterTextSplitter(
        separator='\n',       # Separador entre os chunks
        chunk_size=8000,      # Tamanho m√°ximo de cada chunk
        chunk_overlap=1600    # Sobreposi√ß√£o entre os chunks
    )
    texts = text_splitter.split_text(text)
    return texts

def create_embeddings(texts):
    """
    Cria embeddings para os textos e constr√≥i um √≠ndice FAISS para busca eficiente.

    Args:
        texts (list): Lista de textos para os quais ser√£o criados embeddings.

    Returns:
        tuple: Objeto de embeddings e o √≠ndice FAISS criado.
    """
    embeddings = OpenAIEmbeddings()
    doc_embeddings = embeddings.embed_documents(texts)    # Cria embeddings para os documentos
    dimension = len(doc_embeddings[0])                    # Obt√©m a dimens√£o dos embeddings
    index = faiss.IndexFlatL2(dimension)                  # Cria um √≠ndice FAISS usando dist√¢ncia L2
    index.add(np.array(doc_embeddings))                   # Adiciona os embeddings ao √≠ndice
    return embeddings, index

def search_docs(query, embeddings, index, texts, k=3):
    """
    Busca os documentos mais relevantes para a consulta fornecida.

    Args:
        query (str): Consulta de busca.
        embeddings (OpenAIEmbeddings): Objeto de embeddings para criar o embedding da consulta.
        index (faiss.IndexFlatL2): √çndice FAISS para realizar a busca.
        texts (list): Lista de textos originais.
        k (int, opcional): N√∫mero de resultados a retornar. Padr√£o √© 3.

    Returns:
        list: Lista dos textos mais relevantes encontrados.
    """
    query_embedding = embeddings.embed_query(query)       # Cria o embedding da consulta
    distances, indices = index.search(np.array([query_embedding]), k)  # Realiza a busca no √≠ndice
    results = [texts[i] for i in indices[0]]              # Recupera os textos correspondentes aos √≠ndices
    return results

def generate_answer(messages, embeddings, index, texts):
    """
    Gera a resposta do chatbot usando o modelo de linguagem, considerando o contexto.

    Args:
        messages (list): Hist√≥rico de mensagens da conversa.
        embeddings (OpenAIEmbeddings): Objeto de embeddings para busca.
        index (faiss.IndexFlatL2): √çndice FAISS para busca.
        texts (list): Lista de textos originais para contexto.

    Returns:
        str: Resposta gerada pelo modelo.
    """
    question = messages[-1]["content"]                    # Obt√©m a √∫ltima pergunta do usu√°rio
    context = search_docs(question, embeddings, index, texts)  # Busca o contexto relevante
    context_str = "\n\n".join(context)                    # Combina os textos do contexto em uma string

    # Configura a mensagem inicial para o modelo
    api_messages = [
        {"role": "system", "content": "Voc√™ √© um assistente especializado no Vestibular da Unicamp 2025. Responda √†s perguntas dos usu√°rios usando apenas as informa√ß√µes do edital oficial fornecido. Seja preciso e direto. Se n√£o encontrar a resposta no contexto, indique que n√£o possui essa informa√ß√£o."},
    ]

    # Seleciona as √∫ltimas tr√™s mensagens do hist√≥rico, se existirem
    previous_messages = messages[-3:] if len(messages) >= 3 else messages
    for msg in previous_messages[:-1]:
        api_messages.append(msg)

    # Adiciona a √∫ltima mensagem do usu√°rio com o contexto encontrado
    last_user_message = messages[-1]
    last_user_message_with_context = {
        "role": last_user_message["role"],
        "content": f"Contexto:\n{context_str}\n\nPergunta:\n{last_user_message['content']}"
    }
    api_messages.append(last_user_message_with_context)

    # Realiza a chamada √† API de completions do OpenAI
    chat_completion = client.chat.completions.create(
        model="gpt-3.5-turbo",            # Modelo a ser utilizado
        messages=api_messages,            # Hist√≥rico de mensagens
        max_tokens=400,                   # M√°ximo de tokens na resposta
        temperature=0.3                   # Controla a aleatoriedade da resposta
    )
    answer = chat_completion.choices[0].message.content.strip()  # Extrai e limpa a resposta gerada
    return answer

def main():
    """
    Fun√ß√£o principal que executa o aplicativo Streamlit.
    """

    # Configura√ß√µes iniciais da p√°gina do Streamlit
    st.set_page_config(page_title="Chatbot Vestibular Unicamp 2025", page_icon="üéì", layout="wide")
    current_dir = os.path.dirname(os.path.abspath(__file__))

    # Adiciona estilo customizado para melhorar a interface
    st.markdown("""
        <style>
            .stChatMessage {
                font-size: 1.1em;
            }
            .user {
                background-color: #DCF8C6;
            }
            .assistant {
                background-color: #F1F0F0;
            }
            footer {visibility: hidden;}
        </style>
        """, unsafe_allow_html=True)

    # Cria a barra lateral com informa√ß√µes adicionais
    with st.sidebar:
        image_path = os.path.join(current_dir, '..', 'data', 'UNICAMP.png')
        st.image(image_path, use_container_width=True)
        st.markdown("## Sobre")
        st.write("Este chatbot foi desenvolvido para ajudar candidatos a esclarecer d√∫vidas sobre o Vestibular da Unicamp 2025.")
        st.markdown("---")
        st.markdown("### Desenvolvido por")
        st.write("Augusto Zolet")
        st.write("[LinkedIn](www.linkedin.com/in/augusto-zolet) | [GitHub](https://github.com/Augusto-Zolet)")

    # T√≠tulo e descri√ß√£o do aplicativo
    st.title("üéì Chatbot Vestibular Unicamp 2025")
    st.write("Bem-vindo ao chatbot que responde suas d√∫vidas sobre o Vestibular da Unicamp 2025! Digite sua pergunta abaixo e aguarde a resposta.")

    # Processa o edital e cria os embeddings na primeira execu√ß√£o
    if 'texts' not in st.session_state:
        with st.spinner('Processando o edital, por favor aguarde...'):
            text_path = os.path.join(current_dir, '..', 'data', 'Normas_Vestibular_2025.txt')
            with open(text_path, 'r', encoding='utf-8') as file:
                edital_text = file.read()
            sentences = preprocess_text(edital_text)               # Pr√©-processa o texto do edital
            texts = split_text(edital_text)                        # Divide o texto em chunks
            embeddings, index = create_embeddings(texts)           # Cria embeddings e √≠ndice FAISS
            st.session_state.texts = texts                         # Armazena os textos no estado da sess√£o
            st.session_state.embeddings = embeddings               # Armazena os embeddings
            st.session_state.index = index                         # Armazena o √≠ndice FAISS
    else:
        # Recupera dados do estado da sess√£o para evitar reprocessamento
        texts = st.session_state.texts
        embeddings = st.session_state.embeddings
        index = st.session_state.index

    # Inicializa o hist√≥rico de mensagens se ainda n√£o existir
    if 'messages' not in st.session_state:
        st.session_state.messages = []

    # Exibe as mensagens anteriores na interface de chat
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Campo de entrada para a pergunta do usu√°rio
    question = st.chat_input("Digite sua pergunta sobre o vestibular:")

    if question:
        # Adiciona a mensagem do usu√°rio ao hist√≥rico
        st.session_state.messages.append({"role": "user", "content": question})
        with st.chat_message("user"):
            st.markdown(f"**Voc√™:** {question}")

        # Gera a resposta do chatbot com base na pergunta
        with st.spinner('Gerando resposta...'):
            try:
                answer = generate_answer(st.session_state.messages, embeddings, index, texts)
                # Adiciona a resposta ao hist√≥rico de mensagens
                st.session_state.messages.append({"role": "assistant", "content": answer})
                with st.chat_message("assistant"):
                    st.markdown(f"**Chatbot:** {answer}")
            except Exception as e:
                st.error(f"Ocorreu um erro: {e}")

# Executa a fun√ß√£o principal quando o script √© executado
if __name__ == '__main__':
    main()
